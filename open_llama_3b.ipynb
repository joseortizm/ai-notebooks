{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import LlamaForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at openlm-research/open_llama_3b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 3200)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'openlm-research/open_llama_3b' # https://huggingface.co/openlm-research/open_llama_3b\n",
    "num_labels = 4 # 'Positive', 'Neutral', 'Negative', 'Irrelevant' (https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis/versions/2)\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, torch_dtype=torch.float16, device_map='mps')\n",
    "# Tokenize the input: Use the AutoTokenizer class from the transformers library to tokenize your input text. Set the tokenizer options according to your classification task.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    pad_token=\"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer)) # https://github.com/huggingface/transformers/issues/1805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(32001, 3200)\n"
     ]
    }
   ],
   "source": [
    "# Accede a la última capa principal\n",
    "#last_layer = model.classifier\n",
    "# Muestra la última capa principal\n",
    "#print(last_layer)\n",
    "last_hidden_layer = model.get_input_embeddings()\n",
    "print(last_hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 3200)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
      "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
      "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=3200, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterDFTrain = pd.read_csv(\"../datasets/twitter/twitter_training.csv\")\n",
    "twitterDFVal = pd.read_csv(\"../datasets/twitter/twitter_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p7/8znh32vx4lg3zl5dx2zw2wbm0000gn/T/ipykernel_2049/3090023879.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitterDFTrain[\"sentiment\"][twitterDFTrain[\"Positive\"]==keyStr] = twitterClasses[keyStr]\n",
      "/var/folders/p7/8znh32vx4lg3zl5dx2zw2wbm0000gn/T/ipykernel_2049/3090023879.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitterDFVal[\"sentiment\"][twitterDFVal[\"Irrelevant\"]==keyStr] = twitterClasses[keyStr]\n"
     ]
    }
   ],
   "source": [
    "twitterClasses = {'Negative':0, 'Positive':1, 'Neutral':2, 'Irrelevant':3}\n",
    "twitterDFTrain[\"sentiment\"] = 0\n",
    "twitterDFVal[\"sentiment\"] = 0\n",
    "for keyStr in twitterClasses.keys():\n",
    "    twitterDFTrain[\"sentiment\"][twitterDFTrain[\"Positive\"]==keyStr] = twitterClasses[keyStr]\n",
    "    twitterDFVal[\"sentiment\"][twitterDFVal[\"Irrelevant\"]==keyStr] = twitterClasses[keyStr]\n",
    "\n",
    "twitterDFTrain[\"text\"] = twitterDFTrain[list(twitterDFTrain.columns)[3]]\n",
    "twitterDFVal[\"text\"] = twitterDFVal[list(twitterDFVal.columns)[3]]\n",
    "twitterDFTrain = twitterDFTrain[[\"text\", \"sentiment\"]] # We only need the tweets and their sentiments for training LLaMA\n",
    "twitterDFVal = twitterDFVal[[\"text\", \"sentiment\"]] # We only need the tweets and their sentiments for training LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(text):\n",
    "    # Capture swear words that are **** out return text\n",
    "    text=re.sub(r'https?://www\\.\\S+\\.com','',text)\n",
    "    text=re.sub(r'[^A-Za-z|\\s]','',text)\n",
    "    text=re.sub(r'\\*+','swear',text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\" #symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\" #transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\" #flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\" \n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_multiplechars(text):\n",
    "    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def clean(df):\n",
    "    for col in ['text']:#,'selected_text']:\n",
    "        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n",
    "        df[col]=df[col].astype(str).apply(lambda x:remove_emoji(x))\n",
    "        df[col]=df[col].astype(str).apply(lambda x:remove_html(x))\n",
    "        df[col]=df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterDFTrain_clean = clean(twitterDFTrain)\n",
    "twitterDFVal_clean = clean(twitterDFVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im getting on borderlands and i will kill you all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im getting on borderlands  and i will murder y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      I am coming to the borders and I will kill you...          1\n",
       "1      im getting on borderlands and i will kill you all          1\n",
       "2      im coming on borderlands and i will murder you...          1\n",
       "3      im getting on borderlands  and i will murder y...          1\n",
       "4      im getting into borderlands and i can murder y...          1\n",
       "...                                                  ...        ...\n",
       "74676  Just realized that the Windows partition of my...          1\n",
       "74677  Just realized that my Mac window partition is ...          1\n",
       "74678  Just realized the windows partition of my Mac ...          1\n",
       "74679  Just realized between the windows partition of...          1\n",
       "74680  Just like the windows partition of my Mac is l...          1\n",
       "\n",
       "[74681 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterDFTrain_clean # train set contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBC News  Amazon boss Jeff Bezos rejects claim...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft Why do I pay for WORD when it functi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi EAHelp Ive had Madeleine McCann in my cella...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Toronto is the arts and culture capital of Ca...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Today sucked so its time to drink wine n play ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Bought a fraction of Microsoft today Small wins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Johnson  Johnson to stop selling talc baby pow...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "0    BBC News  Amazon boss Jeff Bezos rejects claim...          2\n",
       "1    Microsoft Why do I pay for WORD when it functi...          0\n",
       "2    CSGO matchmaking is so full of closet hacking ...          0\n",
       "3    Now the President is slapping Americans in the...          2\n",
       "4    Hi EAHelp Ive had Madeleine McCann in my cella...          0\n",
       "..                                                 ...        ...\n",
       "994   Toronto is the arts and culture capital of Ca...          3\n",
       "995  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...          3\n",
       "996  Today sucked so its time to drink wine n play ...          1\n",
       "997    Bought a fraction of Microsoft today Small wins          1\n",
       "998  Johnson  Johnson to stop selling talc baby pow...          2\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "twitterDFVal_clean # validation set contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando dataset con el mismo del archivo LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, twitterDF):\n",
    "        self.twitterDF = twitterDF\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.twitterDF.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.array([idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomDataset(twitterDFTrain_clean)\n",
    "validation_data = CustomDataset(twitterDFVal_clean)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CustomDataset"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando dataset con el mismo del archivo analisis senti BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crearemos esta clase para que bert pueda leerlos (cada reviw)\n",
    "# CREACIÓN DATASET(heredamos Dataset de Pytorch)\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self,reviews,labels,tokenizer,max_len):\n",
    "        self.reviews = reviews #comentarios\n",
    "        self.labels = labels #si es positivo o negativo\n",
    "        self.tokenizer = tokenizer #lo que nos permite convertir el texto en tokens(formato de entrada que requiere bert)\n",
    "        self.max_len = max_len #Longitud mmaxima\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews) #longitud del set de datos\n",
    "  \n",
    "  #cuando estemos creando los paquetes de 16 datos, pytorch ira llamando a esta funcion\n",
    "  #para presentar al modelo\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])#toma un review(texto en bruto)\n",
    "        label = self.labels[item] #toma su labels (etiqueta)\n",
    "        #convertir en representacion numerica(con encode_plus) el texto en bruto\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length = self.max_len,\n",
    "            truncation = True,\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids = False,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )   \n",
    "        #devuelva esta funcion: el review, inputs id(representacion numerica de cada token, etiqueta)\n",
    "        return {\n",
    "              'review': review,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten(),\n",
    "              'label': torch.tensor(label, dtype=torch.long)\n",
    "          } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro Data loader (es quien llamara a la clase IMDBDataset):\n",
    "#Recordar que el review y label deben de convertirse en numpy\n",
    "#num_workers: para que haga 4 procesos a la vez. En nuestro caso si caca batch_size tiene 16 datos\n",
    "#entonces analizara de 4 en 4...\n",
    "#Detalle: i\n",
    "#-se convierte a numpyt cada review y label del dataset\n",
    "#-se usa el tokenizer que creamos y viene del bert pre entrenado\n",
    "\n",
    "def data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = IMDBDataset(\n",
    "    reviews = df.text.to_numpy(),\n",
    "    labels = df.sentiment.to_numpy(),\n",
    "    tokenizer = tokenizer,\n",
    "    max_len = MAX_LEN\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = twitterDFTrain_clean\n",
    "RANDOM_SEED = 42\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    pad_token=\"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "#dividir nuestro set de datos(train_test_split ya lo habiamos importado antes)\n",
    "df_train, df_test = train_test_split(df, test_size = 0.2, random_state=RANDOM_SEED)\n",
    "#creamos los dataloader:\n",
    "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x147497e50>\n"
     ]
    }
   ],
   "source": [
    "print(train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'openlm-research/open_llama_3b'\n",
    "num_labels = 4\n",
    "class LLamaSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(LLamaSentimentClassifier, self).__init__()\n",
    "        #self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) #bert pre entrenado\n",
    "        self.llama = LLaMaForSequenceClassification.from_pretrained(model_path, num_labels=n_classes, torch_dtype=torch.float16, device_map='mps')\n",
    "        #self.llama = LLaMaForSequenceClassification.from_pretrained(model_path)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.llama.config.hidden_size, n_classes) #tendra una capa lineal\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #primero paso los datos por bert. Me devolvera dos datos:\n",
    "        #todos los vectores correspondientes a la codificacion de las frases de entrada\n",
    "        #codificacion del token de clasificacion(el que nos interesa) y \n",
    "        #tiene toda la escencia de la frase y procesa la red neuronal\n",
    "        _, cls_output = self.llama(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        drop_output = self.drop(cls_output)\n",
    "        output = self.linear(drop_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLaMaForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LlamaModel \u001b[39m=\u001b[39m LLamaSentimentClassifier(num_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LlamaModel  \u001b[39m=\u001b[39m LlamaModel\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39msuper\u001b[39m(LLamaSentimentClassifier, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) #bert pre entrenado\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#self.llama = LLaMaForSequenceClassification.from_pretrained(model_path, num_labels=n_classes, torch_dtype=torch.float16, device_map='mps')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllama \u001b[39m=\u001b[39m LLaMaForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jose/Documents/Streaming/YouTube/ai-notebooks/open_llama_3b.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllama\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mhidden_size, n_classes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLaMaForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "LlamaModel = LLamaSentimentClassifier(num_labels)\n",
    "LlamaModel  = LlamaModel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar los layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    }
   ],
   "source": [
    "total_layers = len(list(model.modules()))\n",
    "print(total_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight \t torch.Size([32001, 3200])\n",
      "model.layers.0.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.0.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.0.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.0.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.0.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.0.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.0.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.0.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.0.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.1.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.1.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.1.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.1.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.1.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.1.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.1.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.1.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.1.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.2.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.2.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.2.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.2.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.2.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.2.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.2.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.2.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.2.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.3.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.3.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.3.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.3.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.3.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.3.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.3.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.3.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.3.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.4.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.4.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.4.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.4.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.4.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.4.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.4.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.4.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.4.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.5.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.5.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.5.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.5.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.5.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.5.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.5.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.5.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.5.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.6.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.6.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.6.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.6.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.6.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.6.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.6.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.6.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.6.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.7.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.7.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.7.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.7.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.7.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.7.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.7.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.7.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.7.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.8.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.8.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.8.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.8.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.8.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.8.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.8.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.8.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.8.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.9.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.9.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.9.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.9.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.9.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.9.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.9.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.9.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.9.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.10.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.10.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.10.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.10.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.10.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.10.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.10.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.10.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.10.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.11.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.11.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.11.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.11.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.11.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.11.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.11.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.11.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.11.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.12.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.12.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.12.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.12.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.12.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.12.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.12.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.12.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.12.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.13.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.13.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.13.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.13.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.13.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.13.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.13.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.13.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.13.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.14.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.14.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.14.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.14.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.14.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.14.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.14.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.14.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.14.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.15.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.15.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.15.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.15.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.15.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.15.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.15.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.15.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.15.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.16.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.16.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.16.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.16.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.16.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.16.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.16.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.16.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.16.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.17.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.17.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.17.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.17.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.17.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.17.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.17.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.17.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.17.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.18.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.18.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.18.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.18.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.18.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.18.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.18.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.18.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.18.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.19.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.19.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.19.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.19.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.19.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.19.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.19.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.19.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.19.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.20.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.20.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.20.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.20.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.20.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.20.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.20.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.20.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.20.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.21.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.21.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.21.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.21.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.21.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.21.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.21.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.21.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.21.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.22.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.22.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.22.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.22.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.22.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.22.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.22.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.22.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.22.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.23.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.23.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.23.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.23.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.23.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.23.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.23.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.23.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.23.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.24.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.24.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.24.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.24.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.24.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.24.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.24.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.24.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.24.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.25.self_attn.q_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.25.self_attn.k_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.25.self_attn.v_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.25.self_attn.o_proj.weight \t torch.Size([3200, 3200])\n",
      "model.layers.25.mlp.gate_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.25.mlp.up_proj.weight \t torch.Size([8640, 3200])\n",
      "model.layers.25.mlp.down_proj.weight \t torch.Size([3200, 8640])\n",
      "model.layers.25.input_layernorm.weight \t torch.Size([3200])\n",
      "model.layers.25.post_attention_layernorm.weight \t torch.Size([3200])\n",
      "model.norm.weight \t torch.Size([3200])\n",
      "score.weight \t torch.Size([4, 3200])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─LlamaModel: 1-1                             --\n",
      "|    └─Embedding: 2-1                         102,403,200\n",
      "|    └─ModuleList: 2-2                        --\n",
      "|    |    └─LlamaDecoderLayer: 3-1            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-2            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-3            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-4            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-5            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-6            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-7            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-8            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-9            123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-10           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-11           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-12           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-13           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-14           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-15           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-16           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-17           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-18           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-19           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-20           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-21           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-22           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-23           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-24           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-25           123,910,400\n",
      "|    |    └─LlamaDecoderLayer: 3-26           123,910,400\n",
      "|    └─LlamaRMSNorm: 2-3                      3,200\n",
      "├─Linear: 1-2                                 12,800\n",
      "======================================================================\n",
      "Total params: 3,324,089,600\n",
      "Trainable params: 3,324,089,600\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─LlamaModel: 1-1                             --\n",
       "|    └─Embedding: 2-1                         102,403,200\n",
       "|    └─ModuleList: 2-2                        --\n",
       "|    |    └─LlamaDecoderLayer: 3-1            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-2            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-3            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-4            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-5            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-6            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-7            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-8            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-9            123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-10           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-11           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-12           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-13           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-14           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-15           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-16           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-17           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-18           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-19           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-20           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-21           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-22           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-23           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-24           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-25           123,910,400\n",
       "|    |    └─LlamaDecoderLayer: 3-26           123,910,400\n",
       "|    └─LlamaRMSNorm: 2-3                      3,200\n",
       "├─Linear: 1-2                                 12,800\n",
       "======================================================================\n",
       "Total params: 3,324,089,600\n",
       "Trainable params: 3,324,089,600\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar en archivo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo mostraba la info limitada que se da desde colab \n",
    "import sys \n",
    "#Copy code\n",
    "# Supongamos que 'model' es tu modelo PyTorch\n",
    "# Especifica el nombre del archivo en el que deseas guardar la representación textual del modelo\n",
    "archivo_guardado = 'modelo.txt'\n",
    "\n",
    "# Abre el archivo en modo escritura y redirige la salida estándar hacia el archivo\n",
    "with open(archivo_guardado, 'w') as archivo:\n",
    "    original_stdout = sys.stdout  # Guarda la salida estándar original\n",
    "    sys.stdout = archivo  # Redirige la salida estándar al archivo\n",
    "    print(model)  # Imprime el modelo (la salida se redirige al archivo)\n",
    "    sys.stdout = original_stdout  # Restaura la salida estándar original\n",
    "\n",
    "# El modelo ha sido impreso en el archivo especificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasaron mas de 1H y no finalizaba\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "# Supongamos que 'model' es tu modelo PyTorch\n",
    "# Especifica el nombre del archivo en el que deseas guardar la información completa\n",
    "archivo_guardado = 'summary_modelo.txt'\n",
    "\n",
    "# Abre el archivo en modo escritura y redirige la salida estándar hacia el archivo\n",
    "with open(archivo_guardado, 'w') as archivo:\n",
    "    original_stdout = sys.stdout  # Guarda la salida estándar original\n",
    "    sys.stdout = archivo  # Redirige la salida estándar al archivo\n",
    "    summary(model, (input_size,))  # Genera y muestra la información del modelo\n",
    "    sys.stdout = original_stdout  # Restaura la salida estándar original\n",
    "\n",
    "# La información completa del modelo se guarda en el archivo especificado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # https://github.com/huggingface/peft/blob/632997d1fb776c3cf05d8c2537ac9a98a7ce9435/src/peft/utils/other.py#L202\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 3200)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=3200, out_features=3200, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3200, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3200, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=3200, out_features=3200, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3200, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3200, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "              (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "              (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): Linear(in_features=3200, out_features=4, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788\n"
     ]
    }
   ],
   "source": [
    "total_layers_lora = len(list(lora_model.modules()))\n",
    "print(total_layers_lora)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
